---
layout: post
title: "New GRA Position"
date: 2025-08-23 09:00:00 -0400
categories: HPC Slurm GRA ARCC Computing Math AI Linux
excerpt: "Wherein I talk about my position as a GRA Research Cyberinfrastructure Facilitator"
---

# Introduction

As part of my research, I use HPC resources very often and this is done through UCF ARCC. This is essentially UCF's super computer and they have tons of storage, tons of cpu power, and tons of gpu power. This makes it super effective for me to get my research done quickly and easily. When I found out about a new position opening up as a GRA Research Cyberinfrastructure Facilitator, I decided to apply not only so that I could help others work as effectively as I had, but also to learn as much as possible about these amazing resources. 


I have only just started this past week but my official duties include:

* Advising on HPC usage, data management, and analysis techniques
* Supporting scientific computing and ML applications in research workflows
* Promoting best practices in coding, scripting, and library management (Python, Linux,
SLURM)
* Assisting in the development of onboarding materials, user guides, and documentation of
best practices
* Delivering workshops, training sessions, and walkthroughs for advanced computing
systems

I am planning on giving a workshop on data visualization at the end of the semester and another one on CUDA usage in the Spring. Thus far, my duties have also included comparing Julia to MATLAB for common numerical tasks in both ease of use and speed. Clearly Julia is faster but is the speed worth the trade off when factoring in how easy MATLAB can be to write in and how much time it might take a researcher to learn to convert their code base into Julia? This is part of my job to figure out. 

I look forward to learning lots of new things and being even more effective when using linux, slurm, and HPC tools in general. :D